\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Caption \relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:storyintro}{{1}{1}{Caption \relax }{figure.caption.1}{}}
\citation{brown2020language,rakshin2020plotmachines}
\citation{yao2018storyline,fan2019structure,ippolito2019rarewords,chandu2020narrative,rakshin2020plotmachines}
\citation{biomotif}
\citation{reimers2019sbert}
\citation{yao2018storyline,ippolito2019rarewords}
\citation{fan2019structure}
\citation{chandu2020narrative}
\citation{papalampidi2020screenplay}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\citation{ippolito2019infill}
\citation{uber2019plug}
\citation{wengong2020moleculemotif}
\citation{alayrac2015align}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background: Pairwise Sequence Alignment}{3}{section.3}}
\newlabel{eqn:align}{{1}{3}{Background: Pairwise Sequence Alignment}{equation.3.1}{}}
\citation{fengdoolittle}
\citation{fengdoolittle}
\@writefile{toc}{\contentsline {section}{\numberline {4}Problem Setup: Multiple Sequence Alignment}{4}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methods}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Progressive Alignment}{4}{subsection.5.1}}
\newlabel{eqn:dplus}{{5}{4}{Progressive Alignment}{equation.5.5}{}}
\citation{petitjean2011dba}
\newlabel{alg:progressive}{{1}{5}{Progressive Alignment \relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Progressive Alignment \relax }}{5}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}An Iterative Averaging Algorithm}{5}{subsection.5.2}}
\newlabel{alg:ia}{{2}{5}{Iterative Averaging Alignment \relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Iterative Averaging Alignment \relax }}{5}{algorithm.2}}
\citation{fan2018writingprompts}
\citation{reimers2019sbert}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hill Climbing Algorithm}{6}{subsection.5.3}}
\newlabel{alg:hillclimbing}{{3}{6}{Hill Climbing Alignment \relax }{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  Hill Climbing Alignment \relax }}{6}{algorithm.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{6}{section.6}}
\citation{bowman2015snli,williams2017mnli}
\citation{fan2018writingprompts}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{7}{section.7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces   The SP scores and Steiner distances for each of the MSA algorithms. The progressive algorithm optimizes the SP score and achieves the lowest. The hill climbing algorithm initialized with the mean sequence obtained from IA obtains the lowest Steiner distance. We see the algorithms get stuck in local optima, as the hill climbing algorithm initialize with the progressive mean sequence obtains a much higher Steiner distance than if it had been initialized with IA. \relax }}{7}{table.caption.2}}
\newlabel{tbl:sp_steiner}{{1}{7}{The SP scores and Steiner distances for each of the MSA algorithms. The progressive algorithm optimizes the SP score and achieves the lowest. The hill climbing algorithm initialized with the mean sequence obtained from IA obtains the lowest Steiner distance. We see the algorithms get stuck in local optima, as the hill climbing algorithm initialize with the progressive mean sequence obtains a much higher Steiner distance than if it had been initialized with IA. \relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   The progressive algorithm outputs alignments that are much sparser and longer than the best-performing method that optimizes the Steiner distance, hill climbing (IA). This is shown by a large number of columns from progressive alignments containing only a single non-gap element, and few columns containing more than 3 non-gap elements. These column density counts were obtained across all multiple alignments from the initial 50 clusters. \relax }}{8}{figure.caption.3}}
\newlabel{fig:colcounts}{{2}{8}{The progressive algorithm outputs alignments that are much sparser and longer than the best-performing method that optimizes the Steiner distance, hill climbing (IA). This is shown by a large number of columns from progressive alignments containing only a single non-gap element, and few columns containing more than 3 non-gap elements. These column density counts were obtained across all multiple alignments from the initial 50 clusters. \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Longer sentences tend to be farther from their nearest neighbours. $2^{16}$ random sentences were sampled from \textsc  {WritingPrompts}, and the plot shows their length versus the average distance to their 10 nearest neighbours. Distances were averaged across all sentences of the same length. \relax }}{8}{figure.caption.4}}
\newlabel{fig:len-v-dist}{{3}{8}{Longer sentences tend to be farther from their nearest neighbours. $2^{16}$ random sentences were sampled from \textsc {WritingPrompts}, and the plot shows their length versus the average distance to their 10 nearest neighbours. Distances were averaged across all sentences of the same length. \relax }{figure.caption.4}{}}
\citation{ippolito2020better}
\citation{alayrac2015align}
\citation{paulus2020gradient}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{9}{section.8}}
\bibstyle{plainnat}
\bibdata{bib}
\bibcite{alayrac2015align}{{1}{2015}{{Alayrac et~al.}}{{Alayrac, Bojanowski, Agrawal, Sivic, Laptev, and Lacoste{-}Julien}}}
\bibcite{bowman2015snli}{{2}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{brown2020language}{{3}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}}}
\bibcite{chandu2020narrative}{{4}{2020}{{Chandu et~al.}}{{Chandu, Dong, and Black}}}
\bibcite{uber2019plug}{{5}{2019}{{Dathathri et~al.}}{{Dathathri, Madotto, Lan, Hung, Frank, Molino, Yosinski, and Liu}}}
\bibcite{biomotif}{{6}{2006}{{D'haeseleer}}{{}}}
\bibcite{fan2018writingprompts}{{7}{2018}{{Fan et~al.}}{{Fan, Lewis, and Dauphin}}}
\bibcite{fan2019structure}{{8}{2019}{{Fan et~al.}}{{Fan, Lewis, and Dauphin}}}
\bibcite{fengdoolittle}{{9}{1987}{{{Feng} and {Doolittle}}}{{}}}
\bibcite{ippolito2019infill}{{10}{2019{}}{{Ippolito et~al.}}{{Ippolito, Grangier, Callison-Burch, and Eck}}}
\bibcite{ippolito2019rarewords}{{11}{2019{}}{{Ippolito et~al.}}{{Ippolito, Grangier, Callison-Burch, and Eck}}}
\bibcite{ippolito2020better}{{12}{2020}{{Ippolito et~al.}}{{Ippolito, Grangier, Eck, and Callison-Burch}}}
\bibcite{wengong2020moleculemotif}{{13}{2020}{{Jin et~al.}}{{Jin, Barzilay, and Jaakkola}}}
\bibcite{papalampidi2020screenplay}{{14}{2020}{{Papalampidi et~al.}}{{Papalampidi, Keller, Frermann, and Lapata}}}
\bibcite{paulus2020gradient}{{15}{2020}{{Paulus et~al.}}{{Paulus, Choi, Tarlow, Krause, and Maddison}}}
\bibcite{petitjean2011dba}{{16}{2011}{{Petitjean et~al.}}{{Petitjean, Ketterlin, and Gan{\c {c}}arski}}}
\bibcite{reimers2019sbert}{{17}{2019}{{Reimers and Gurevych}}{{}}}
\bibcite{williams2017mnli}{{18}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{yao2018storyline}{{19}{2018}{{Yao et~al.}}{{Yao, Peng, Weischedel, Knight, Zhao, and Yan}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   The first few sentences from each of the 5 stories in our space story cluster analysis. The prompt is for each story is given in italics. All stories have a space-related theme. The first two sentences of story 5 have been filtered out, as they repeated the prompt. \relax }}{12}{figure.caption.5}}
\newlabel{fig:story-starts}{{4}{12}{The first few sentences from each of the 5 stories in our space story cluster analysis. The prompt is for each story is given in italics. All stories have a space-related theme. The first two sentences of story 5 have been filtered out, as they repeated the prompt. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   The multiple story alignment of the space story cluster, obtained via progressive alignment. We refer to the story in the top-most row as story 1, down through the bottom-most row as story 5. Each colored square corresponds to a sentence from the corresponding story, while a dark square corresponds to a gap. Sentences within a row proceed in order of their appearance in the story; only gaps are inserted within each row. An identical dummy beginning-of-story sentence token was inserted at the start of each story, yielding the full column of matches at index 0. \relax }}{13}{figure.caption.6}}
\newlabel{fig:msa}{{5}{13}{The multiple story alignment of the space story cluster, obtained via progressive alignment. We refer to the story in the top-most row as story 1, down through the bottom-most row as story 5. Each colored square corresponds to a sentence from the corresponding story, while a dark square corresponds to a gap. Sentences within a row proceed in order of their appearance in the story; only gaps are inserted within each row. An identical dummy beginning-of-story sentence token was inserted at the start of each story, yielding the full column of matches at index 0. \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   An example of a good alignment between 4 story segments from the multiple alignment. The first sentences from stories 1, 2, and 5 detail an event which sparked the following sentences. The next sentences from all four stories mention a form of exploration, investigation, or expansion. \relax }}{13}{figure.caption.7}}
\newlabel{fig:good-alignment}{{6}{13}{An example of a good alignment between 4 story segments from the multiple alignment. The first sentences from stories 1, 2, and 5 detail an event which sparked the following sentences. The next sentences from all four stories mention a form of exploration, investigation, or expansion. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   An example of a poor alignment between two story segments from the multiple alignment. The sentences are weak semantic matches, although both segments serve as exposition in their respective stories. \relax }}{14}{figure.caption.8}}
\newlabel{fig:bad-alignment}{{7}{14}{An example of a poor alignment between two story segments from the multiple alignment. The sentences are weak semantic matches, although both segments serve as exposition in their respective stories. \relax }{figure.caption.8}{}}
