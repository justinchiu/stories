
@ARTICLE{petitjean2011dba,
  title={A global averaging method for dynamic time warping, with applications to clustering},
  author={Petitjean, Fran{\c{c}}ois and Ketterlin, Alain and Gan{\c{c}}arski, Pierre},
  journal={Pattern Recognition},
  volume={44},
  number={3},
  pages={678--693},
  year={2011},
  publisher={Elsevier}
}

@article{fengdoolittle,
    author={DF {Feng} and RF {Doolittle}},
    title={Progressive sequence alignment as a prerequisite to correct phylogenetic trees},
    year = {1987},
    booktitle = {J Mol Evol},
}

@inbook{gusfield1997, place={Cambridge}, title={Multiple String Comparison – The Holy Grail}, DOI={10.1017/CBO9780511574931.017}, booktitle={Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology}, publisher={Cambridge University Press}, author={Gusfield, Dan}, year={1997}, pages={332–369}}

@inproceedings{lei2004triangle,
author = {Chen, Lei and Ng, Raymond},
title = {On the Marriage of Lp-Norms and Edit Distance},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Existing studies on time series are based on two categories of distance functions. The first category consists of the Lp-norms. They are metric distance functions but cannot support local time shifting. The second category consists of distance functions which are capable of handling local time shifting but are nonmetric. The first contribution of this paper is the proposal of a new distance function, which we call ERP ("Edit distance with Real Penalty"). Representing a marriage of L1- norm and the edit distance, ERP can support local time shifting, and is a metric.The second contribution of the paper is the development of pruning strategies for large time series databases. Given that ERP is a metric, one way to prune is to apply the triangle inequality. Another way to prune is to develop a lower bound on the ERP distance. We propose such a lower bound, which has the nice computational property that it can be efficiently indexed with a standard B+- tree. Moreover, we show that these two ways of pruning can be used simultaneously for ERP distances. Specifically, the false positives obtained from the B+-tree can be further minimized by applying the triangle inequality. Based on extensive experimentation with existing benchmarks and techniques, we show that this combination delivers superb pruning power and search time performance, and dominates all existing strategies.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {792–803},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@InProceedings{cuturi2017sdtw,
  title = 	 {Soft-{DTW}: a Differentiable Loss Function for Time-Series},
  author = 	 {Marco Cuturi and Mathieu Blondel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {894--903},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/cuturi17a/cuturi17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/cuturi17a.html},
  abstract = 	 {We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a \emph{differentiable} loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at https://github.com/mblondel/soft-dtw}
}

@ARTICLE{zhou2016gctw,

  author={F. {Zhou} and F. {De la Torre}},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Generalized Canonical Time Warping}, 

  year={2016},

  volume={38},

  number={2},

pages={279-294},}

@inproceedings{reimers2019sbert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

% writingprompts

@article{fan2018writingprompts,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Hierarchical Neural Story Generation},
  journal   = {CoRR},
  volume    = {abs/1805.04833},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04833},
  archivePrefix = {arXiv},
  eprint    = {1805.04833},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04833.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fan2019structure,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Strategies for Structuring Story Generation},
  journal   = {CoRR},
  volume    = {abs/1902.01109},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.01109},
  archivePrefix = {arXiv},
  eprint    = {1902.01109},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-01109.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yao2018storyline,
  author    = {Lili Yao and
               Nanyun Peng and
               Ralph M. Weischedel and
               Kevin Knight and
               Dongyan Zhao and
               Rui Yan},
  title     = {Plan-And-Write: Towards Better Automatic Storytelling},
  journal   = {CoRR},
  volume    = {abs/1811.05701},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.05701},
  archivePrefix = {arXiv},
  eprint    = {1811.05701},
  timestamp = {Sat, 31 Aug 2019 16:23:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-05701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% old

@article{shakir2019mcgrad,
  title={Monte Carlo Gradient Estimation in Machine Learning},
  author={Shakir Mohamed and Mihaela Rosca and Michael Figurnov and Andriy Mnih},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.10652}
}

@article{maddison2016poissonprocess,
  title={A Poisson process model for Monte Carlo},
  author={Chris J. Maddison},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.05986}
}

@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}

@article{grathwohl2017relax,
  author    = {Will Grathwohl and
               Dami Choi and
               Yuhuai Wu and
               Geoffrey Roeder and
               David Duvenaud},
  title     = {Backpropagation through the Void: Optimizing control variates for
               black-box gradient estimation},
  journal   = {CoRR},
  volume    = {abs/1711.00123},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00123},
  archivePrefix = {arXiv},
  eprint    = {1711.00123},
  timestamp = {Mon, 22 Jul 2019 14:09:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-00123.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paulus2020sst,
  title={Gradient Estimation with Stochastic Softmax Tricks},
  author={Max B. Paulus and Dami Choi and Daniel Tarlow and Andreas Krause
    and Chris J. Maddison},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.08063}
}

@inproceedings{xu2018varrp,
  title={Variance Reduction Properties of the Reparameterization Trick},
  author={Ming Xu and Matias Quiroz and Robert Kohn and Scott Anthony Sisson},
  booktitle={AISTATS},
  year={2018}
}

@article{williams2017mnli,
  author    = {Adina Williams and
               Nikita Nangia and
               Samuel R. Bowman},
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
               Inference},
  journal   = {CoRR},
  volume    = {abs/1704.05426},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05426},
  archivePrefix = {arXiv},
  eprint    = {1704.05426},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WilliamsNB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bowman2015snli,
  author    = {Samuel R. Bowman and
               Gabor Angeli and
               Christopher Potts and
               Christopher D. Manning},
  title     = {A large annotated corpus for learning natural language inference},
  journal   = {CoRR},
  volume    = {abs/1508.05326},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.05326},
  archivePrefix = {arXiv},
  eprint    = {1508.05326},
  timestamp = {Mon, 13 Aug 2018 16:46:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BowmanAPM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
