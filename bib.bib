
@article{alayrac2015align,
  author    = {Jean{-}Baptiste Alayrac and
               Piotr Bojanowski and
               Nishant Agrawal and
               Josef Sivic and
               Ivan Laptev and
               Simon Lacoste{-}Julien},
  title     = {Learning from narrated instruction videos},
  journal   = {CoRR},
  volume    = {abs/1506.09215},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.09215},
  archivePrefix = {arXiv},
  eprint    = {1506.09215},
  timestamp = {Mon, 13 Aug 2018 16:49:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AlayracBASLL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{petitjean2011dba,
  title={A global averaging method for dynamic time warping, with applications to clustering},
  author={Petitjean, Fran{\c{c}}ois and Ketterlin, Alain and Gan{\c{c}}arski, Pierre},
  journal={Pattern Recognition},
  volume={44},
  number={3},
  pages={678--693},
  year={2011},
  publisher={Elsevier}
}

@article{fengdoolittle,
    author={DF {Feng} and RF {Doolittle}},
    title={Progressive sequence alignment as a prerequisite to correct phylogenetic trees},
    year = {1987},
    booktitle = {J Mol Evol},
}

@inbook{gusfield1997, place={Cambridge}, title={Multiple String Comparison – The Holy Grail}, DOI={10.1017/CBO9780511574931.017}, booktitle={Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology}, publisher={Cambridge University Press}, author={Gusfield, Dan}, year={1997}, pages={332–369}}

@inproceedings{lei2004triangle,
author = {Chen, Lei and Ng, Raymond},
title = {On the Marriage of Lp-Norms and Edit Distance},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Existing studies on time series are based on two categories of distance functions. The first category consists of the Lp-norms. They are metric distance functions but cannot support local time shifting. The second category consists of distance functions which are capable of handling local time shifting but are nonmetric. The first contribution of this paper is the proposal of a new distance function, which we call ERP ("Edit distance with Real Penalty"). Representing a marriage of L1- norm and the edit distance, ERP can support local time shifting, and is a metric.The second contribution of the paper is the development of pruning strategies for large time series databases. Given that ERP is a metric, one way to prune is to apply the triangle inequality. Another way to prune is to develop a lower bound on the ERP distance. We propose such a lower bound, which has the nice computational property that it can be efficiently indexed with a standard B+- tree. Moreover, we show that these two ways of pruning can be used simultaneously for ERP distances. Specifically, the false positives obtained from the B+-tree can be further minimized by applying the triangle inequality. Based on extensive experimentation with existing benchmarks and techniques, we show that this combination delivers superb pruning power and search time performance, and dominates all existing strategies.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {792–803},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@InProceedings{cuturi2017sdtw,
  title = 	 {Soft-{DTW}: a Differentiable Loss Function for Time-Series},
  author = 	 {Marco Cuturi and Mathieu Blondel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {894--903},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/cuturi17a/cuturi17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/cuturi17a.html},
  abstract = 	 {We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a \emph{differentiable} loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at https://github.com/mblondel/soft-dtw}
}

@ARTICLE{zhou2016gctw,

  author={F. {Zhou} and F. {De la Torre}},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Generalized Canonical Time Warping}, 

  year={2016},

  volume={38},

  number={2},

pages={279-294},}

@inproceedings{reimers2019sbert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

% writingprompts

@article{fan2018writingprompts,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Hierarchical Neural Story Generation},
  journal   = {CoRR},
  volume    = {abs/1805.04833},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04833},
  archivePrefix = {arXiv},
  eprint    = {1805.04833},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04833.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fan2019structure,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Strategies for Structuring Story Generation},
  journal   = {CoRR},
  volume    = {abs/1902.01109},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.01109},
  archivePrefix = {arXiv},
  eprint    = {1902.01109},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-01109.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yao2018storyline,
  author    = {Lili Yao and
               Nanyun Peng and
               Ralph M. Weischedel and
               Kevin Knight and
               Dongyan Zhao and
               Rui Yan},
  title     = {Plan-And-Write: Towards Better Automatic Storytelling},
  journal   = {CoRR},
  volume    = {abs/1811.05701},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.05701},
  archivePrefix = {arXiv},
  eprint    = {1811.05701},
  timestamp = {Sat, 31 Aug 2019 16:23:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-05701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% old

@article{shakir2019mcgrad,
  title={Monte Carlo Gradient Estimation in Machine Learning},
  author={Shakir Mohamed and Mihaela Rosca and Michael Figurnov and Andriy Mnih},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.10652}
}

@article{maddison2016poissonprocess,
  title={A Poisson process model for Monte Carlo},
  author={Chris J. Maddison},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.05986}
}

@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}

@article{grathwohl2017relax,
  author    = {Will Grathwohl and
               Dami Choi and
               Yuhuai Wu and
               Geoffrey Roeder and
               David Duvenaud},
  title     = {Backpropagation through the Void: Optimizing control variates for
               black-box gradient estimation},
  journal   = {CoRR},
  volume    = {abs/1711.00123},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00123},
  archivePrefix = {arXiv},
  eprint    = {1711.00123},
  timestamp = {Mon, 22 Jul 2019 14:09:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-00123.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paulus2020sst,
  title={Gradient Estimation with Stochastic Softmax Tricks},
  author={Max B. Paulus and Dami Choi and Daniel Tarlow and Andreas Krause
    and Chris J. Maddison},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.08063}
}

@inproceedings{xu2018varrp,
  title={Variance Reduction Properties of the Reparameterization Trick},
  author={Ming Xu and Matias Quiroz and Robert Kohn and Scott Anthony Sisson},
  booktitle={AISTATS},
  year={2018}
}

@article{williams2017mnli,
  author    = {Adina Williams and
               Nikita Nangia and
               Samuel R. Bowman},
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
               Inference},
  journal   = {CoRR},
  volume    = {abs/1704.05426},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05426},
  archivePrefix = {arXiv},
  eprint    = {1704.05426},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WilliamsNB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bowman2015snli,
  author    = {Samuel R. Bowman and
               Gabor Angeli and
               Christopher Potts and
               Christopher D. Manning},
  title     = {A large annotated corpus for learning natural language inference},
  journal   = {CoRR},
  volume    = {abs/1508.05326},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.05326},
  archivePrefix = {arXiv},
  eprint    = {1508.05326},
  timestamp = {Mon, 13 Aug 2018 16:46:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BowmanAPM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{biomotif,
    author = {Patrik D'haeseleer},
    title = {What are DNA sequence motifs?},
    journal = {Nature Biotechnology},
    year = {2006},
    url = {https://rdcu.be/b9Co9},
}

@unknown{wengong2020moleculemotif,
author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
year = {2020},
month = {02},
pages = {},
title = {Hierarchical Generation of Molecular Graphs using Structural Motifs}
}

@inproceedings{ippolito2019rarewords,
    title = "Unsupervised Hierarchical Story Infilling",
    author = "Ippolito, Daphne  and
      Grangier, David  and
      Callison-Burch, Chris  and
      Eck, Douglas",
    booktitle = "Proceedings of the First Workshop on Narrative Understanding",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-2405",
    doi = "10.18653/v1/W19-2405",
    pages = "37--43",
    abstract = "Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.",
}

@article{kececioglu2000polyhedral,
abstract = {We study two new problems in sequence alignment both from a practical and a theoretical view, using tools from combinatorial optimization to develop branch-and-cut algorithms. The generalized maximum trace formulation captures several forms of multiple sequence alignment problems in a common framework, among them the original formulation of maximum trace. The RNA sequence alignment problem captures the comparison of RNA molecules on the basis of their primary sequence and their secondary structure. Both problems have a characterization in terms of graphs which we reformulate in terms of integer linear programming. We then study the polytopes (or convex hulls of all feasible solutions) associated with the integer linear program for both problems. For each polytope we derive several classes of facet-defining inequalities and show that for some of these classes the corresponding separation problem can be solved in polynomial time. This leads to a polynomial-time algorithm for pairwise sequence alignment that is not based on dynamic programming. Moreover, for multiple sequences the branch-and-cut algorithms for both sequence alignment problems are able to solve to optimality instances that are beyond the range of present dynamic programming approaches.},
author = {Kececioglu, John D and Lenhof, Hans-Peter and Mehlhorn, Kurt and Mutzel, Petra and Reinert, Knut and Vingron, Martin},
doi = {https://doi.org/10.1016/S0166-218X(00)00194-3},
issn = {0166-218X},
journal = {Discrete Applied Mathematics},
keywords = { Branch-and-cut, Combinatorial optimization, Multiple sequence alignment, RNA sequence alignment,Computational biology},
number = {1},
pages = {143--186},
title = {{A polyhedral approach to sequence alignment problems}},
url = {http://www.sciencedirect.com/science/article/pii/S0166218X00001943},
volume = {104},
year = {2000}
}

@article{chandu2020narrative,
  author    = {Khyathi Raghavi Chandu and
               Ruo{-}Ping Dong and
               Alan W. Black},
  title     = {Reading Between the Lines: Exploring Infilling in Visual Narratives},
  journal   = {CoRR},
  volume    = {abs/2010.13944},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.13944},
  archivePrefix = {arXiv},
  eprint    = {2010.13944},
  timestamp = {Mon, 02 Nov 2020 18:17:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-13944.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ippolito2020better,
      title={Toward Better Storylines with Sentence-Level Language Models}, 
      author={Daphne Ippolito and David Grangier and Douglas Eck and Chris Callison-Burch},
      year={2020},
      eprint={2005.05255},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ippolito2019infill,
    title = "Unsupervised Hierarchical Story Infilling",
    author = "Ippolito, Daphne  and
      Grangier, David  and
      Callison-Burch, Chris  and
      Eck, Douglas",
    booktitle = "Proceedings of the First Workshop on Narrative Understanding",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-2405",
    doi = "10.18653/v1/W19-2405",
    pages = "37--43",
    abstract = "Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.",
}

@article{uber2019plug,
  author    = {Sumanth Dathathri and
               Andrea Madotto and
               Janice Lan and
               Jane Hung and
               Eric Frank and
               Piero Molino and
               Jason Yosinski and
               Rosanne Liu},
  title     = {Plug and Play Language Models: {A} Simple Approach to Controlled Text
               Generation},
  journal   = {CoRR},
  volume    = {abs/1912.02164},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.02164},
  archivePrefix = {arXiv},
  eprint    = {1912.02164},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-02164.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{paulus2020gradient,
      title={Gradient Estimation with Stochastic Softmax Tricks}, 
      author={Max B. Paulus and Dami Choi and Daniel Tarlow and Andreas Krause and Chris J. Maddison},
      year={2020},
      eprint={2006.08063},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{papalampidi2020screenplay,
    title = "Screenplay Summarization Using Latent Narrative Structure",
    author = "Papalampidi, Pinelopi  and
      Keller, Frank  and
      Frermann, Lea  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.174",
    doi = "10.18653/v1/2020.acl-main.174",
    pages = "1920--1933",
    abstract = "Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.",
}

@inproceedings{rashkin2020plotmachines,
    title = "{P}lot{M}achines: Outline-Conditioned Generation with Dynamic Plot State Tracking",
    author = "Rashkin, Hannah  and
      Celikyilmaz, Asli  and
      Choi, Yejin  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.349",
    pages = "4274--4295",
    abstract = "We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",
}
