
@inproceedings{reimers2019sbert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

% writingprompts

@article{fan2018writingprompts,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Hierarchical Neural Story Generation},
  journal   = {CoRR},
  volume    = {abs/1805.04833},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04833},
  archivePrefix = {arXiv},
  eprint    = {1805.04833},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04833.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fan2019structure,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Strategies for Structuring Story Generation},
  journal   = {CoRR},
  volume    = {abs/1902.01109},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.01109},
  archivePrefix = {arXiv},
  eprint    = {1902.01109},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-01109.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yao2018storyline,
  author    = {Lili Yao and
               Nanyun Peng and
               Ralph M. Weischedel and
               Kevin Knight and
               Dongyan Zhao and
               Rui Yan},
  title     = {Plan-And-Write: Towards Better Automatic Storytelling},
  journal   = {CoRR},
  volume    = {abs/1811.05701},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.05701},
  archivePrefix = {arXiv},
  eprint    = {1811.05701},
  timestamp = {Sat, 31 Aug 2019 16:23:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-05701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% old

@article{shakir2019mcgrad,
  title={Monte Carlo Gradient Estimation in Machine Learning},
  author={Shakir Mohamed and Mihaela Rosca and Michael Figurnov and Andriy Mnih},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.10652}
}

@article{maddison2016poissonprocess,
  title={A Poisson process model for Monte Carlo},
  author={Chris J. Maddison},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.05986}
}

@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}

@article{grathwohl2017relax,
  author    = {Will Grathwohl and
               Dami Choi and
               Yuhuai Wu and
               Geoffrey Roeder and
               David Duvenaud},
  title     = {Backpropagation through the Void: Optimizing control variates for
               black-box gradient estimation},
  journal   = {CoRR},
  volume    = {abs/1711.00123},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00123},
  archivePrefix = {arXiv},
  eprint    = {1711.00123},
  timestamp = {Mon, 22 Jul 2019 14:09:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-00123.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paulus2020sst,
  title={Gradient Estimation with Stochastic Softmax Tricks},
  author={Max B. Paulus and Dami Choi and Daniel Tarlow and Andreas Krause
    and Chris J. Maddison},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.08063}
}

@inproceedings{xu2018varrp,
  title={Variance Reduction Properties of the Reparameterization Trick},
  author={Ming Xu and Matias Quiroz and Robert Kohn and Scott Anthony Sisson},
  booktitle={AISTATS},
  year={2018}
}
