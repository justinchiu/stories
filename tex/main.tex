\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}

\title{Storylines}

\begin{document}
\maketitle

\section{Introduction}
% Improve coherency of generative models of text by discovering a notion of storylines.
Our goal is to improve the coherency of generative story models by modeling storylines.
As an alternative to black-box models with little explicit structure,
prior work has noted that including structure in the generative model improves coherence
in the generations \citep{yao2018storyline,fan2019structure}.

However, prior investigations into narrative structure used hand-crafted representations,
such as entity coreference or keywords.
We hope to alleviate the need for hand-crafted structure via structured generative models.

\section{Storyline Induction}
As an initial exploratory step,
we turn to sequence alignment algorithms to induce segmentations of stories.

Sequence alignment algorithms are used in biology to identify
similar subsequences of proteins in an effort to determine whether a set of
proteins share common ancestry.
The algorithms start with a sequence of atomic representations,
such as amino acids, then build up alignments using these atomic representations.
Context is taken into account not in the representations,
but through the dynamic program.
We posit that these algorithms may also produce motifs that
resemble storylines in short stories.
We perform alignment with each independent sentence representation as an atom.


We will answer the following questions with our exploratory analysis:
\begin{enumerate}
\item At what level should we analyze narrative structure?
    Do storylines have a set or random length?
\item Is there a general structure, such as exposition, complication, falling action, and resolution?
    What are the narrative elements present in short stories?
\item How do we handle gaps that appear in this structure?
\item Can we leverage prompts in structure induction?
\item For assigning a given sentence to a narrative element,
    is the context (surrounding sentences) or sentence itself (ignoring context)
    more important?
\end{enumerate}

%(What if there's too much information in SBERT?
%What information is relevant to a storyline?
%What is a storyline?
%Current approach: random search to identify whether certain methods
%produce storylines)
\begin{comment}
Additionally,
if a parameterized distance measure (with associated representations)
is correlated with human judgements for stories,
then it may be possible to engineer an unsupervised objective that allows
training of the parameterized distance to further improve performance.
This approach to analysis allows us to initially avoid expensive
likelihood-based methods that reconstruct full stories.
\end{comment}

\subsection{Scale}

\section{Pairwise Alignment with SBERT and DTW}
\label{pairwise}
We first investigate whether we can recover structure that resembles a storyline
using pretrained models.
In particular, we use SBERT \citep{reimers2019sbert}
to map sentences $\bx_i \in \mcX^*$ to vector representations $\by_i \in \R^n$.
As each story consists of several sentences, we obtain a sequence of representations
corresponding to each sentence in a given story $\bY = \langle \by_0, \ldots, \by_T \rangle$.
We then compute an alignment between the sentences of two stories using DTW.

We evaluate the efficacy of SBERT and DTW without fine-tuning by determining whether
$DTW(Y_i, Y_j)$ correlates with an intuitive distance of stories.
For a given story we obtain the top $K$ closest stories under DTW
in addition to $K$ additional random stories,
and compare the ranking under DTW with human annotation.

We additionally evaluate the alignments (TBD)

Analysis forthcoming.

\section{Multisequence Alignment}
\label{msa}
Although we found that the pairwise comparison using DTW correlated with intuitive distance,
we aim to further extract robust storylines that are common to multiple stories.
To accomplish this, we turn to multisequence alignment (MSA) which
considers multiple stories and produces a global alignment across
multiple stories.

We use the multisequence generalization of DTW \citep{zhou2016gctw}.
We perform an evaluation of this measure:
For a given collection of stories, we compare the $K$-subset with the lowest mDTW
verus a human-annotated subset.
We obtain a collection of stories by obtaining the $M > K$ closest stories to a given story,
as measured by DTW.

Analysis forthcoming.

\section{A Generative Model}
We wish to improve the accuracy of DTW and mDTW without human annotation.
we optimize a nonparametric time-series modeling objective that extends builds on
pairwise DTW:
\begin{equation*}
\log p(Y_i) = \log \sum_j p(Y_i, Y_j)
\end{equation*}

Details TBD, but will use initial SBERT+DTW distance to approximate integral.
if preliminary evaluation in Sections~\ref{pairwise} and \ref{msa}
are positive, then there is hope that the approximation
grids do not have to be extremely large or updated as training progresses.

We also experiment with the multi-sequence extension.

\newpage
\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
