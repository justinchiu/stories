\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}

\title{Storylines}

\begin{document}
\maketitle

\section{Introduction}
% Improve coherency of generative models of text by discovering a notion of storylines.
Our goal is to improve the coherency of generative story models by modeling storylines.
As an alternative to black-box models with little explicit structure,
prior work has noted that including structure in the generative model improves coherence
in the generations \citep{yao2018storyline,fan2019structure}.

However, prior investigations into narrative structure used hand-crafted representations,
such as entity coreference or keywords.
We hope to alleviate the need for hand-crafted structure via structured generative models.

\section{Storyline Induction}
As an initial step, we turn to sequence alignment algorithms to induce storylines.
If a parameterized distance measure (with associated representations)
is correlated with human judgements for stories,
then it may be possible to engineer an unsupervised objective that allows
training of the parameterized distance to further improve performance.
We may then use the resulting representations to induce interpretable storylines
through a discretization procedure.
This approach to analysis allows us to initially avoid expensive
likelihood-based methods that reconstruct full stories.

(What if there's too much information in SBERT?
What information is relevant to a storyline?
What is a storyline?
Current approach: random search to identify whether certain methods
produce storylines)

\section{Pairwise Alignment with SBERT and DTW}
\label{pairwise}
We first investigate whether we can recover structure that resembles a storyline
using pretrained models.
In particular, we use SBERT \citep{reimers2019sbert}
to map sentences $\bx_i \in \mcX^*$ to vector representations $\by_i \in \R^n$.
As each story consists of several sentences, we obtain a sequence of representations
corresponding to each sentence in a given story $\bY = \langle \by_0, \ldots, \by_T \rangle$.
We then compute an alignment between the sentences of two stories using DTW.

We evaluate the efficacy of SBERT and DTW without fine-tuning by determining whether
$DTW(Y_i, Y_j)$ correlates with an intuitive distance of stories.
For a given story we obtain the top $K$ closest stories under DTW
and compare the ranking with human annotation.

We additionally evaluate the alignments (TBD)

Analysis forthcoming.

\section{Multisequence Alignment}
\label{msa}
Although we found that the pairwise comparison using DTW correlated with intuitive distance,
we aim to further extract robust storylines that are common to multiple stories.
To accomplish this, we turn to multisequence alignment (MSA) which
considers multiple stories and produces a global alignment across
multiple stories.

We use the multisequence generalization of DTW \citep{zhou2016gctw}.
We perform an evaluation of this measure:
For a given collection of stories, we compare the $K$-subset with the lowest mDTW
verus a human-annotated subset.
We obtain a collection of stories by obtaining the $M > K$ closest stories to a given story,
as measured by DTW.

Analysis forthcoming.

\section{A Generative Model}
We wish to improve the accuracy of DTW and mDTW without human annotation.
We optimize a nonparametric time-series modeling objective that extends builds on
pairwise DTW:
\begin{equation*}
\log p(Y_i) = \log \sum_j p(Y_i, Y_j)
\end{equation*}

Details TBD, but will use DTW distance to approximate integral.
If preliminary evaluation in Sections~\ref{pairwise} and \ref{msa}
are positive, then there is hope that the approximation
grids do not have to be extremely large or updated as training progresses.

We also experiment with the multi-sequence extension.

\newpage
\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
